{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pymer4 import Lmer\n",
    "import scipy.stats as stats\n",
    "\n",
    "from significance_analysis import conduct_analysis\n",
    "\n",
    "# Load example dataset\n",
    "data = pd.read_csv(\"./example_dataset.csv\")\n",
    "\n",
    "def GLRT(mod1, mod2):\n",
    "    chi_square = 2 * abs(mod1.logLike - mod2.logLike)\n",
    "    delta_params = abs(len(mod1.coefs) - len(mod2.coefs))\n",
    "    return {\n",
    "        \"chi_square\": chi_square,\n",
    "        \"df\": delta_params,\n",
    "        \"p\": 1 - stats.chi2.cdf(chi_square, df=delta_params),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Analysis: Analyse performance of acquisition functions over all benchmarks and trainingrounds\n",
    "data2=data.loc[(data[\"acquisition\"]!=\"ExpectedImprovement\")| (data[\"benchmark\"]!=\"Branin\") ]\n",
    "#conduct_analysis(data2, \"mean\", \"acquisition\", \"benchmark\",show_plots=False,summarize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# \"Common\"-Model assumes significant difference, which is why the system-identifier is included\\ncomplex_model = Lmer(\\n                formula=f\"{metric}~{system_id}+{meta_cat}+(1|{input_id})\", data=data3\\n            )\\nsimple_model = Lmer(\\n                formula=f\"{metric}~{system_id}+(1|{input_id})\", data=data3\\n            )\\n            \\n\\n#complex_model = Lmer(\\n#                formula=f\"{metric}~{input_id}+{meta_cat}+(1|{system_id})\", data=data3\\n#            )\\n\\nmeta_dim=\"benchmark_dim\"\\nmeta_cat=\"benchmark_cat\"\\ninput_id=\"acquisition\"\\nsystem_id=\"benchmark\"\\ncomplex_model = Lmer(\\n                formula=f\"{metric}~{meta_dim}+{meta_cat}+(1|{input_id})\", data=data3\\n            )\\nsimple_model = Lmer(\\n                formula=f\"{metric}~{meta_dim}+(1|{input_id})\", data=data3\\n            )\\n\\n# factors specifies names of system_identifier, i.e. Baseline, or Algorithm1\\ncomplex_model.fit(\\n    factors={system_id: list(data[system_id].unique())},\\n    REML=False,\\n    summarize=False,\\n)\\nsimple_model.fit(\\n    factors={system_id: list(data[system_id].unique())},\\n    REML=False,\\n    summarize=False,\\n)\\nprint(GLRT(simple_model, complex_model))\\nprint(complex_model.summary())\\nprint(complex_model.ranef)\\nprint(simple_model.summary())\\nprint(simple_model.ranef)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3=data\n",
    "dimension={\n",
    "    \"Branin\":2,\n",
    "    \"Hartmann6\":6,\n",
    "    \"Jahs_Bench\":10,\n",
    "    \"NN_HPO_Bench\":10\n",
    "}\n",
    "categorical={\n",
    "    \"Branin\":\"Numerical\",\n",
    "    \"Hartmann6\":\"Numerical\",\n",
    "    \"Jahs_Bench\":\"Categorical\",\n",
    "    \"NN_HPO_Bench\":\"Numerical\"\n",
    "}\n",
    "data3[\"benchmark_dim\"]=data3[\"benchmark\"].apply(lambda x:dimension[x])\n",
    "data3[\"benchmark_cat\"]=data3[\"benchmark\"].apply(lambda x:categorical[x])\n",
    "\n",
    "\n",
    "metric=\"mean\"\n",
    "input_id=\"benchmark\"\n",
    "system_id=\"acquisition\"\n",
    "bin_id=\"budget\"\n",
    "\"\"\"\n",
    "# \"Common\"-Model assumes significant difference, which is why the system-identifier is included\n",
    "complex_model = Lmer(\n",
    "                formula=f\"{metric}~{system_id}+{meta_cat}+(1|{input_id})\", data=data3\n",
    "            )\n",
    "simple_model = Lmer(\n",
    "                formula=f\"{metric}~{system_id}+(1|{input_id})\", data=data3\n",
    "            )\n",
    "            \n",
    "\n",
    "#complex_model = Lmer(\n",
    "#                formula=f\"{metric}~{input_id}+{meta_cat}+(1|{system_id})\", data=data3\n",
    "#            )\n",
    "\n",
    "meta_dim=\"benchmark_dim\"\n",
    "meta_cat=\"benchmark_cat\"\n",
    "input_id=\"acquisition\"\n",
    "system_id=\"benchmark\"\n",
    "complex_model = Lmer(\n",
    "                formula=f\"{metric}~{meta_dim}+{meta_cat}+(1|{input_id})\", data=data3\n",
    "            )\n",
    "simple_model = Lmer(\n",
    "                formula=f\"{metric}~{meta_dim}+(1|{input_id})\", data=data3\n",
    "            )\n",
    "\n",
    "# factors specifies names of system_identifier, i.e. Baseline, or Algorithm1\n",
    "complex_model.fit(\n",
    "    factors={system_id: list(data[system_id].unique())},\n",
    "    REML=False,\n",
    "    summarize=False,\n",
    ")\n",
    "simple_model.fit(\n",
    "    factors={system_id: list(data[system_id].unique())},\n",
    "    REML=False,\n",
    "    summarize=False,\n",
    ")\n",
    "print(GLRT(simple_model, complex_model))\n",
    "print(complex_model.summary())\n",
    "print(complex_model.ranef)\n",
    "print(simple_model.summary())\n",
    "print(simple_model.ranef)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acqu_dict_fam={'ExpectedImprovement': 'EI', 'ProbabilityOfImprovement': 'PI', 'UpperConfidenceBound': 'UCB', 'qExpectedImprovement': 'EI', 'qKnowledgeGradient': 'KG', 'qProbabilityOfImprovement': 'PI', 'qSimpleRegret': 'SR', 'qUpperConfidenceBound': 'UCB', 'randomSearch': 'RS'}\n",
    "acqu_dict_cat={'ExpectedImprovement': 'AN', 'ProbabilityOfImprovement': 'AN', 'UpperConfidenceBound': 'AN', 'qExpectedImprovement': 'MC', 'qKnowledgeGradient': 'MC', 'qProbabilityOfImprovement': 'MC', 'qSimpleRegret': 'MC', 'qUpperConfidenceBound': 'MC', 'randomSearch': 'AN'}\n",
    "data3[\"acquisition_fam\"]=data3[\"acquisition\"].apply(lambda x:acqu_dict_fam[x])\n",
    "data3[\"acquisition_cat\"]=data3[\"acquisition\"].apply(lambda x:acqu_dict_cat[x])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for ['Branin', 'Hartmann6', 'NN_HPO_Bench']\n",
      "P-value: 1.1102230246251565e-16\n",
      "\n",
      "As the p-value 1.1102230246251565e-16 is smaller than 0.05, we can reject the Null-Hypothesis that the model that does not consider the acquisition describes the data as well as the one that does. Therefore there is significant difference within acquisition.\n",
      "\n",
      "P-values adjusted by tukey method for family of 36 estimates\n",
      "The best performing acquisition is qExpectedImprovement, but ['ExpectedImprovement', 'UpperConfidenceBound', 'qKnowledgeGradient', 'qSimpleRegret', 'qUpperConfidenceBound'] are only insignificantly worse.\n",
      "\n",
      "Analysis for ['Jahs_Bench']\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "P-value: 0.0\n",
      "\n",
      "As the p-value 0.0 is smaller than 0.05, we can reject the Null-Hypothesis that the model that does not consider the acquisition describes the data as well as the one that does. Therefore there is significant difference within acquisition.\n",
      "\n",
      "P-values adjusted by tukey method for family of 36 estimates\n",
      "The best performing acquisition is qSimpleRegret, but ['ExpectedImprovement', 'UpperConfidenceBound'] are only insignificantly worse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categorical_means=conduct_analysis(data3,metric,system_id,input_id,show_plots=False,show_contrasts=False,summarize=False, subset=(input_id,categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for ['Branin']\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "P-value: 2.2334027233172904e-06\n",
      "\n",
      "As the p-value 2.2334027233172904e-06 is smaller than 0.05, we can reject the Null-Hypothesis that the model that does not consider the acquisition describes the data as well as the one that does. Therefore there is significant difference within acquisition.\n",
      "\n",
      "P-values adjusted by tukey method for family of 36 estimates\n",
      "The best performing acquisition is ProbabilityOfImprovement, but ['ExpectedImprovement', 'UpperConfidenceBound', 'qExpectedImprovement', 'qKnowledgeGradient', 'qSimpleRegret', 'qUpperConfidenceBound'] are only insignificantly worse.\n",
      "\n",
      "Analysis for ['Hartmann6']\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "boundary (singular) fit: see help('isSingular') \n",
      "\n",
      "P-value: 0.0\n",
      "\n",
      "As the p-value 0.0 is smaller than 0.05, we can reject the Null-Hypothesis that the model that does not consider the acquisition describes the data as well as the one that does. Therefore there is significant difference within acquisition.\n",
      "\n",
      "P-values adjusted by tukey method for family of 36 estimates\n",
      "The best performing acquisition is qKnowledgeGradient, but ['qExpectedImprovement'] are only insignificantly worse.\n",
      "\n",
      "Analysis for ['Jahs_Bench', 'NN_HPO_Bench']\n",
      "P-value: 0.0\n",
      "\n",
      "As the p-value 0.0 is smaller than 0.05, we can reject the Null-Hypothesis that the model that does not consider the acquisition describes the data as well as the one that does. Therefore there is significant difference within acquisition.\n",
      "\n",
      "P-values adjusted by tukey method for family of 36 estimates\n",
      "The best performing acquisition is qSimpleRegret, but ['ExpectedImprovement', 'UpperConfidenceBound'] are only insignificantly worse.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dimensional_means=conduct_analysis(data3,metric,system_id,input_id,show_plots=False,show_contrasts=False,summarize=False, subset=(input_id,dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 acquisition    mean     SE                  benchmark_group acquisition_fam acquisition_cat\n",
      "1        ExpectedImprovement   0.384  1.416  Branin__Hartmann6__NN_HPO_Bench              EI              AN\n",
      "2   ProbabilityOfImprovement   0.608  1.416  Branin__Hartmann6__NN_HPO_Bench              PI              AN\n",
      "3       UpperConfidenceBound   0.468  1.416  Branin__Hartmann6__NN_HPO_Bench             UCB              AN\n",
      "4       qExpectedImprovement   0.335  1.416  Branin__Hartmann6__NN_HPO_Bench              EI              MC\n",
      "5         qKnowledgeGradient   0.486  1.416  Branin__Hartmann6__NN_HPO_Bench              KG              MC\n",
      "6  qProbabilityOfImprovement   0.774  1.416  Branin__Hartmann6__NN_HPO_Bench              PI              MC\n",
      "7              qSimpleRegret   0.589  1.416  Branin__Hartmann6__NN_HPO_Bench              SR              MC\n",
      "8      qUpperConfidenceBound   0.410  1.416  Branin__Hartmann6__NN_HPO_Bench             UCB              MC\n",
      "9               randomSearch   0.960  1.416  Branin__Hartmann6__NN_HPO_Bench              RS              AN\n",
      "1        ExpectedImprovement  21.886  0.106                       Jahs_Bench              EI              AN\n",
      "2   ProbabilityOfImprovement  22.563  0.106                       Jahs_Bench              PI              AN\n",
      "3       UpperConfidenceBound  22.110  0.106                       Jahs_Bench             UCB              AN\n",
      "4       qExpectedImprovement  22.282  0.106                       Jahs_Bench              EI              MC\n",
      "5         qKnowledgeGradient  23.563  0.106                       Jahs_Bench              KG              MC\n",
      "6  qProbabilityOfImprovement  22.809  0.106                       Jahs_Bench              PI              MC\n",
      "7              qSimpleRegret  21.656  0.106                       Jahs_Bench              SR              MC\n",
      "8      qUpperConfidenceBound  22.418  0.106                       Jahs_Bench             UCB              MC\n",
      "9               randomSearch  23.727  0.106                       Jahs_Bench              RS              AN\n",
      "1        ExpectedImprovement   3.562  0.177                           Branin              EI              AN\n",
      "2   ProbabilityOfImprovement   3.361  0.177                           Branin              PI              AN\n",
      "3       UpperConfidenceBound   3.735  0.177                           Branin             UCB              AN\n",
      "4       qExpectedImprovement   3.465  0.177                           Branin              EI              MC\n",
      "5         qKnowledgeGradient   3.937  0.177                           Branin              KG              MC\n",
      "6  qProbabilityOfImprovement   4.582  0.177                           Branin              PI              MC\n",
      "7              qSimpleRegret   3.702  0.177                           Branin              SR              MC\n",
      "8      qUpperConfidenceBound   3.566  0.177                           Branin             UCB              MC\n",
      "9               randomSearch   4.267  0.177                           Branin              RS              AN\n",
      "1        ExpectedImprovement  -2.413  0.014                        Hartmann6              EI              AN\n",
      "2   ProbabilityOfImprovement  -1.541  0.014                        Hartmann6              PI              AN\n",
      "3       UpperConfidenceBound  -2.335  0.014                        Hartmann6             UCB              AN\n",
      "4       qExpectedImprovement  -2.464  0.014                        Hartmann6              EI              MC\n",
      "5         qKnowledgeGradient  -2.483  0.014                        Hartmann6              KG              MC\n",
      "6  qProbabilityOfImprovement  -2.264  0.014                        Hartmann6              PI              MC\n",
      "7              qSimpleRegret  -1.937  0.014                        Hartmann6              SR              MC\n",
      "8      qUpperConfidenceBound  -2.337  0.014                        Hartmann6             UCB              MC\n",
      "9               randomSearch  -1.390  0.014                        Hartmann6              RS              AN\n",
      "1        ExpectedImprovement  10.945  7.974         Jahs_Bench__NN_HPO_Bench              EI              AN\n",
      "2   ProbabilityOfImprovement  11.283  7.974         Jahs_Bench__NN_HPO_Bench              PI              AN\n",
      "3       UpperConfidenceBound  11.057  7.974         Jahs_Bench__NN_HPO_Bench             UCB              AN\n",
      "4       qExpectedImprovement  11.143  7.974         Jahs_Bench__NN_HPO_Bench              EI              MC\n",
      "5         qKnowledgeGradient  11.783  7.974         Jahs_Bench__NN_HPO_Bench              KG              MC\n",
      "6  qProbabilityOfImprovement  11.407  7.974         Jahs_Bench__NN_HPO_Bench              PI              MC\n",
      "7              qSimpleRegret  10.830  7.974         Jahs_Bench__NN_HPO_Bench              SR              MC\n",
      "8      qUpperConfidenceBound  11.211  7.974         Jahs_Bench__NN_HPO_Bench             UCB              MC\n",
      "9               randomSearch  11.865  7.974         Jahs_Bench__NN_HPO_Bench              RS              AN\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(categorical_means,dict) and isinstance(dimensional_means,dict)\n",
    "training_set=pd.DataFrame(columns=[\"acquisition\",\"mean\",\"SE\"])\n",
    "for category in range(len(categorical_means.keys())):\n",
    "    category_dataset=pd.DataFrame()\n",
    "    category_dataset[[\"acquisition\",\"mean\",\"SE\"]]=categorical_means[list(categorical_means.keys())[category]][1].loc[:,[\"acquisition\",\"Estimate\",\"SE\"]]\n",
    "    category_dataset[\"benchmark_group\"]=list(categorical_means.keys())[category]\n",
    "    training_set=pd.concat([training_set,category_dataset])\n",
    "for category in range(len(dimensional_means.keys())):\n",
    "    category_dataset=pd.DataFrame()\n",
    "    category_dataset[[\"acquisition\",\"mean\",\"SE\"]]=dimensional_means[list(dimensional_means.keys())[category]][1].loc[:,[\"acquisition\",\"Estimate\",\"SE\"]]\n",
    "    category_dataset[\"benchmark_group\"]=list(dimensional_means.keys())[category]\n",
    "    training_set=pd.concat([training_set,category_dataset])\n",
    "training_set[\"acquisition_fam\"]=training_set[\"acquisition\"].apply(lambda x:acqu_dict_fam[x])\n",
    "training_set[\"acquisition_cat\"]=training_set[\"acquisition\"].apply(lambda x:acqu_dict_cat[x])\n",
    "print(training_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step - loss: 0.1196 - accuracy: 1.0000\n",
      "Loss: 0.11963988095521927\n",
      "Accuracy: 1.0\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "[[ 0.2793731   1.382649  ]\n",
      " [ 0.67498803  1.3733635 ]\n",
      " [ 0.46001232  1.3780336 ]\n",
      " [ 0.39606345  1.4025085 ]\n",
      " [ 0.4747377   1.451755  ]\n",
      " [ 0.7299327   1.4330691 ]\n",
      " [ 0.46569258  1.4321262 ]\n",
      " [ 0.3796398   1.4035348 ]\n",
      " [ 1.0583689   1.4520493 ]\n",
      " [21.7602      0.13514015]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>SE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.384</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.608</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.468</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.335</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.486</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.774</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.589</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.410</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.960</td>\n",
       "      <td>1.416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21.886</td>\n",
       "      <td>0.106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean     SE\n",
       "1   0.384  1.416\n",
       "2   0.608  1.416\n",
       "3   0.468  1.416\n",
       "4   0.335  1.416\n",
       "5   0.486  1.416\n",
       "6   0.774  1.416\n",
       "7   0.589  1.416\n",
       "8   0.410  1.416\n",
       "9   0.960  1.416\n",
       "1  21.886  0.106"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(16, activation='relu', input_shape=(None,13)))\n",
    "  model.add(Dense(32, activation='sigmoid'))\n",
    "  model.add(Dense(2, activation='linear'))\n",
    "\n",
    "  model.compile(loss='mse', optimizer='adam',metrics=[\"accuracy\"])\n",
    "  return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "x_data = ohe.fit_transform(training_set[['acquisition_cat', 'acquisition_fam',\"benchmark_group\"]]).toarray()\n",
    "\n",
    "y_data = training_set[['mean', 'SE']]\n",
    "# Train the model\n",
    "model.fit(x_data, y_data, epochs=1000,verbose=0)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_data, y_data)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)\n",
    "print(model.predict(x_data[0:10]))\n",
    "y_data[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
