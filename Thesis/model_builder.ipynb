{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import rankdata\n",
    "from itertools import product,combinations,chain\n",
    "from models import bt_plot,model,glrt\n",
    "from dataset_management import get_dataset\n",
    "from pymer4.models import Lmer\n",
    "\n",
    "\n",
    "pd.set_option(\"chained_assignment\", None)\n",
    "pd.set_option(\"display.max_rows\", 5000)\n",
    "pd.set_option(\"display.max_columns\", 5000)\n",
    "pd.set_option(\"display.width\", 10000)\n",
    "fig5_df=get_dataset(dataset_name=\"fig5_24_at25_bad_intsteps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos=[\"1\",\"2\",\"3\"]\n",
    "seeds=[str(x) for x in range(50)]\n",
    "benchmarks=[\"0\",\"2\",\"3\",\"4\"]\n",
    "budgets=[1,2]\n",
    "combination = list(product(algos, benchmarks, [0],seeds,budgets))\n",
    "\n",
    "random_df = pd.DataFrame(combination, columns=[\"algorithm\", \"benchmark\", \"value\", \"seed\",\"budget\"])\n",
    "random_df[\"value\"] = random_df.apply(lambda x: np.random.normal(0, 0.1),axis=1)\n",
    "\n",
    "algo_by_seed_df = pd.DataFrame(combination, columns=[\"algorithm\", \"benchmark\", \"value\", \"seed\",\"budget\"])\n",
    "algo_by_seed_df[\"value\"] = algo_by_seed_df.apply(lambda row:np.random.normal(int(row[\"seed\"])*0.5 if row[\"algorithm\"] in [\"1\",\"2\"] and int(row[\"seed\"])%5==0 else 0.5, 0.1), axis=1)\n",
    "\n",
    "\n",
    "algo_by_benchmark_df = pd.DataFrame(combination, columns=[\"algorithm\", \"benchmark\", \"value\", \"seed\",\"budget\"])\n",
    "algo_by_benchmark_df[\"value\"] = algo_by_benchmark_df.apply(lambda row: np.random.normal(0.5*int(row[\"algorithm\"])*int(row[\"benchmark\"]), 0.01), axis=1)\n",
    "\n",
    "\n",
    "value_by_budget_df=pd.DataFrame(combination, columns=[\"algorithm\", \"benchmark\", \"value\", \"seed\",\"budget\"])\n",
    "value_by_budget_df[\"value\"] = value_by_budget_df.apply(lambda row:np.random.normal(0.5*int(row[\"budget\"]), 0.1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple (-244290.11) << fidelity (-243312.56)\n",
      "Chi-Square: 1955.0920213969657, P-Value: 0.0\n",
      "simple (-244290.11) << fidelity_group (-243001.25)\n",
      "Chi-Square: 2577.7240843258332, P-Value: 0.0\n",
      "fidelity (-243312.56) << fidelity_group (-243001.25)\n",
      "Chi-Square: 622.6320629288675, P-Value: 0.0\n",
      "value ~  algorithm + (1|benchmark) + used_fidelity + algorithm:used_fidelity\n",
      "fidelity (-243312.56) << fidelity_both (-243001.25)\n",
      "Chi-Square: 622.6320629238035, P-Value: 0.0\n",
      "Fidelity used_fidelity as single and interaction effect are both significant, but interaction is more significant.\n",
      "value ~  + algorithm + (1|benchmark) + algorithm:used_fidelity\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class model_builder():\n",
    "    def __init__(self,df:pd.DataFrame,loss_var:str=\"value\",system_var=\"algorithm\",benchmark_var=\"benchmark\",fidelities=None):\n",
    "        self.df=df\n",
    "        self.loss_formula=f\"{loss_var} ~ \"\n",
    "        self.exploratory_var=system_var\n",
    "        self.benchmark_var=benchmark_var\n",
    "        self.fidelities=fidelities\n",
    "        self.fidelity_sig={f: -1 for f in self.fidelities}\n",
    "\n",
    "    def test_seed_dependency(self,verbose:bool=False):\n",
    "        simpel_model=model(formula=f\"{self.loss_formula}+{self.exploratory_var}\",data=self.df,factor_list=[self.exploratory_var],dummy=False)\n",
    "        seed_model=model(formula=f\"{self.loss_formula}+(0+{self.exploratory_var}|seed)\",data=self.df,factor_list=[self.exploratory_var],dummy=False)\n",
    "        test_result=glrt(simpel_model,seed_model,names=[\"simple\",\"seed\"] if verbose else None,returns=True)\n",
    "        if test_result[\"p\"]<0.05 and seed_model.logLike>simpel_model.logLike:\n",
    "            ranef_var=seed_model.ranef_var\n",
    "            print(f\"Seed is significant, likely influenced algorithms: {ranef_var.loc[(ranef_var['Var']/10 >= ranef_var['Var'].min())&(ranef_var.index!='Residual')&(ranef_var['Var']*10 >= ranef_var['Var'].max())]['Name'].to_list()}\")\n",
    "            return ranef_var.loc[(ranef_var['Var']/10 >= ranef_var['Var'].min())&(ranef_var.index!='Residual')&(ranef_var['Var']*10 >= ranef_var['Var'].max())]['Name'].to_list()\n",
    "        else:\n",
    "            print(\"Seed is not significant\")\n",
    "            return []\n",
    "\n",
    "    def test_benchmark_information(self,rank_benchmarks:bool=False,verbose:bool=False):\n",
    "        test_results={}\n",
    "        benchmark_info={}\n",
    "        for benchmark in self.df[self.benchmark_var].unique():\n",
    "            simple_mod=model(formula=f\"{self.loss_formula}+1\",data=self.df.loc[self.df[self.benchmark_var]==benchmark],factor_list=[self.exploratory_var],dummy=False)\n",
    "            benchmark_mod=model(formula=f\"{self.loss_formula}+{self.exploratory_var}\",data=self.df.loc[self.df[self.benchmark_var]==benchmark],factor_list=[self.exploratory_var],dummy=False)\n",
    "            if verbose:\n",
    "                print(f\"\\nBenchmark: {benchmark}\")\n",
    "            test_results[benchmark]=glrt(simple_mod,benchmark_mod,names=[\"simple\",\"algorithm\"] if verbose else None,returns=True)\n",
    "            if test_results[benchmark][\"p\"]<0.05 and benchmark_mod.logLike>simple_mod.logLike:\n",
    "                print(f\"Benchmark {benchmark} is informative.\")\n",
    "                benchmark_info[benchmark]=True\n",
    "            else:\n",
    "                print(f\"Benchmark {benchmark} is uninformative.\")\n",
    "                benchmark_info[benchmark]=False\n",
    "        if any(test_results[b][\"p\"]>0.05 for b in test_results.keys()):\n",
    "            if rank_benchmarks:\n",
    "                all_benchmarks_mod=model(formula=f\"{self.loss_formula}+(0+{self.benchmark_var}|{self.exploratory_var})\",data=self.df,factor_list=[self.exploratory_var],dummy=False)\n",
    "                print(all_benchmarks_mod.ranef_var)\n",
    "\n",
    "    def test_fidelity(self, fidelity_var:str,verbose:bool=False):\n",
    "        significances={fidelity_var:0,f\"{fidelity_var}_group\":0}\n",
    "        simple_formula=f\"{self.loss_formula} {self.exploratory_var}{f' + (1|{self.benchmark_var})' if self.df[self.benchmark_var].nunique()>1 else ''}\"\n",
    "        simple_mod=model(formula=simple_formula,data=self.df,factor_list=[self.exploratory_var],dummy=self.df[self.benchmark_var].nunique()==1)\n",
    "        fidelity_mod=model(formula=f\"{simple_formula} + {fidelity_var}\",data=self.df,factor_list=[self.exploratory_var],dummy=self.df[self.benchmark_var].nunique()==1)\n",
    "        test_result=glrt(simple_mod,fidelity_mod,names=[\"simple\",\"fidelity\"] if verbose else None,returns=True)\n",
    "        if test_result[\"p\"]<0.05 and fidelity_mod.logLike>simple_mod.logLike:\n",
    "            significances[fidelity_var]=1\n",
    "        fid_group_mod=model(formula=f\"{simple_formula} + {self.exploratory_var}:{fidelity_var}\",data=self.df,factor_list=[self.exploratory_var],dummy=self.df[self.benchmark_var].nunique()==1)\n",
    "        test_result=glrt(simple_mod,fid_group_mod,names=[\"simple\",\"fidelity_group\"] if verbose else None,returns=True)\n",
    "        if test_result[\"p\"]<0.05 and fid_group_mod.logLike>simple_mod.logLike:\n",
    "            significances[f\"{fidelity_var}_group\"]=1\n",
    "        if significances[fidelity_var]==1 and significances[f\"{fidelity_var}_group\"]==1:\n",
    "            test_result=glrt(fidelity_mod,fid_group_mod,names=[\"fidelity\",\"fidelity_group\"] if verbose else None,returns=True)\n",
    "            if test_result[\"p\"]<0.05 and fid_group_mod.logLike>fidelity_mod.logLike:\n",
    "                if verbose:\n",
    "                    print(f\"Fidelity {fidelity_var} as single and interaction effect are both significant, but interaction is more significant.\")\n",
    "                self.fidelity_sig[fidelity_var]=2\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Fidelity {fidelity_var} as single and interaction effect both significant, but as single factor is more significant.\")\n",
    "                self.fidelity_sig[fidelity_var]=1\n",
    "        elif significances[fidelity_var]==1:\n",
    "            if verbose:\n",
    "                print(f\"Fidelity {fidelity_var} as single factor significant.\")\n",
    "            self.fidelity_sig[fidelity_var]= 1\n",
    "        elif significances[f\"{fidelity_var}_group\"]==1:\n",
    "            if verbose:\n",
    "                print(f\"Fidelity {fidelity_var} as interaction is significant.\")\n",
    "            self.fidelity_sig[fidelity_var]= 2\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Fidelity {fidelity_var} is not significant.\")\n",
    "            self.fidelity_sig[fidelity_var]= 0\n",
    "\n",
    "    def full_test(self, verbose:bool=False):\n",
    "        self.test_seed_dependency(verbose=verbose)\n",
    "        self.test_benchmark_information(verbose=verbose)\n",
    "        for f in self.fidelities:\n",
    "            self.test_fidelity(f,verbose=verbose)\n",
    "\n",
    "    def build_model(self):\n",
    "        for fidelity,sig in self.fidelity_sig.items():\n",
    "            if sig==-1:\n",
    "                self.test_fidelity(fidelity,verbose=True)\n",
    "        model_formula=f\"{self.loss_formula} + {self.exploratory_var} + (1|{self.benchmark_var})\"+\"\".join([f\" + {self.exploratory_var}:{f}\" if self.fidelity_sig[f]==2 else f\" + {f}\" for f in self.fidelities if self.fidelity_sig[f]>0])\n",
    "        print(model_formula)\n",
    "\n",
    "builder=model_builder(fig5_df,system_var=\"algorithm\",benchmark_var=\"benchmark\",fidelities=[\"used_fidelity\"])\n",
    "builder.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': ['PB', 'RS', 'HB'], 'used_fidelity_group': ['1.0', '2.0', '3.0', '4.0', '5.0']}\n",
      "Fitting linear model using lmer with Wald confidence intervals...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: fixed-effect model matrix is rank deficient so dropping 1 column / coefficient\n",
      "\n",
      "R[write to console]: fixed-effect model matrix is rank deficient so dropping 1 column / coefficient\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear mixed model fit by maximum likelihood  ['lmerMod']\n",
      "Formula: value~algorithm+used_fidelity+used_fidelity_group+algorithm:used_fidelity_group+(1|benchmark)\n",
      "\n",
      "Family: gaussian\t Inference: parametric\n",
      "\n",
      "Number of observations: 18000\t Groups: {'benchmark': 12.0}\n",
      "\n",
      "Log-likelihood: -62793.362 \t AIC: 125620.724\n",
      "\n",
      "Random effects:\n",
      "\n",
      "                  Name     Var    Std\n",
      "benchmark  (Intercept)  35.643  5.970\n",
      "Residual                62.468  7.904\n",
      "\n",
      "No random effect correlations specified\n",
      "\n",
      "Fixed effects:\n",
      "\n",
      "                                 Estimate  2.5_ci  97.5_ci     SE         DF  T-stat  P-val  Sig\n",
      "(Intercept)                        16.021  12.595   19.447  1.748     12.664   9.166    0.0  ***\n",
      "algorithm1                         -9.578 -10.210   -8.945  0.323  17988.000 -29.683    0.0  ***\n",
      "algorithm2                         -6.308  -6.940   -5.675  0.323  17988.000 -19.549    0.0  ***\n",
      "used_fidelity                      -2.781  -2.939   -2.623  0.081  17988.000 -34.476    0.0  ***\n",
      "used_fidelity_group1               -5.806  -6.376   -5.236  0.291  17988.000 -19.963    0.0  ***\n",
      "used_fidelity_group2               -4.790  -5.337   -4.242  0.279  17988.000 -17.141    0.0  ***\n",
      "used_fidelity_group3               -2.775  -3.346   -2.205  0.291  17988.000  -9.543    0.0  ***\n",
      "algorithm1:used_fidelity_group1     7.684   6.789    8.578  0.456  17988.000  16.839    0.0  ***\n",
      "algorithm2:used_fidelity_group1     5.250   4.355    6.144  0.456  17988.000  11.505    0.0  ***\n",
      "algorithm1:used_fidelity_group2     9.147   8.252   10.041  0.456  17988.000  20.044    0.0  ***\n",
      "algorithm2:used_fidelity_group2     5.606   4.712    6.500  0.456  17988.000  12.285    0.0  ***\n",
      "algorithm1:used_fidelity_group3     9.753   8.858   10.647  0.456  17988.000  21.372    0.0  ***\n",
      "algorithm2:used_fidelity_group3     6.367   5.472    7.261  0.456  17988.000  13.953    0.0  ***\n",
      "algorithm1:used_fidelity_group4     9.700   8.806   10.595  0.456  17988.000  21.258    0.0  ***\n",
      "algorithm2:used_fidelity_group4     6.365   5.471    7.259  0.456  17988.000  13.948    0.0  ***\n",
      "              (Intercept)  algorithm1  algorithm2  used_fidelity  used_fidelity_group1  used_fidelity_group2  used_fidelity_group3  algorithm1:used_fidelity_group1  algorithm2:used_fidelity_group1  algorithm1:used_fidelity_group2  algorithm2:used_fidelity_group2  algorithm1:used_fidelity_group3  algorithm2:used_fidelity_group3  algorithm1:used_fidelity_group4  algorithm2:used_fidelity_group4\n",
      "JAHS-C10        29.892599   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "JAHS-CH         26.051938   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "JAHS-FM         21.944561   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "LC-126026       12.396232   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "LC-167190       12.549049   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "LC-168330       12.786560   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "LC-168910       12.722936   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "LC-189906       12.575830   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "PD1-Cifar100    12.796590   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "PD1-ImageNet    12.716938   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "PD1-LM1B        13.011481   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "PD1-WMT         12.808676   -9.577767   -6.307832      -2.781051             -5.806287             -4.789763             -2.775465                         7.683804                         5.249834                         9.146552                         5.606113                         9.752533                         6.366848                         9.700468                         6.364952\n",
      "P-values adjusted by tukey method for family of 3 estimates\n",
      "(   algorithm used_fidelity_group  Estimate  2.5_ci  97.5_ci     SE      DF\n",
      "1         PB                 1.0       NaN     NaN      NaN    NaN     NaN\n",
      "2         RS                 1.0       NaN     NaN      NaN    NaN     NaN\n",
      "3         HB                 1.0       NaN     NaN      NaN    NaN     NaN\n",
      "4         PB                 2.0       NaN     NaN      NaN    NaN     NaN\n",
      "5         RS                 2.0       NaN     NaN      NaN    NaN     NaN\n",
      "6         HB                 2.0       NaN     NaN      NaN    NaN     NaN\n",
      "7         PB                 3.0     2.888  -0.886    6.663  1.738  12.395\n",
      "8         RS                 3.0     2.457  -1.317    6.231  1.738  12.395\n",
      "9         HB                 3.0     2.186  -1.588    5.961  1.738  12.395\n",
      "10        PB                 4.0       NaN     NaN      NaN    NaN     NaN\n",
      "11        RS                 4.0       NaN     NaN      NaN    NaN     NaN\n",
      "12        HB                 4.0       NaN     NaN      NaN    NaN     NaN\n",
      "13        PB                 5.0       NaN     NaN      NaN    NaN     NaN\n",
      "14        RS                 5.0       NaN     NaN      NaN    NaN     NaN\n",
      "15        HB                 5.0       NaN     NaN      NaN    NaN     NaN,    Contrast used_fidelity_group  Estimate  2.5_ci  97.5_ci     SE       DF  T-stat  P-val  Sig\n",
      "1   PB - RS                 1.0     9.578   8.821   10.334  0.323  17988.0  29.683  0.000  ***\n",
      "2   PB - HB                 1.0     6.308   5.552    7.064  0.323  17988.0  19.549  0.000  ***\n",
      "3   RS - HB                 1.0    -3.270  -4.026   -2.514  0.323  17988.0 -10.134  0.000  ***\n",
      "4   PB - RS                 2.0     1.894   1.138    2.650  0.323  17988.0   5.870  0.000  ***\n",
      "5   PB - HB                 2.0     1.058   0.302    1.814  0.323  17988.0   3.279  0.003   **\n",
      "6   RS - HB                 2.0    -0.836  -1.592   -0.080  0.323  17988.0  -2.591  0.026    *\n",
      "7   PB - RS                 3.0     0.431  -0.325    1.188  0.323  17988.0   1.336  0.375     \n",
      "8   PB - HB                 3.0     0.702  -0.055    1.458  0.323  17988.0   2.175  0.076    .\n",
      "9   RS - HB                 3.0     0.271  -0.486    1.027  0.323  17988.0   0.838  0.679     \n",
      "10  PB - RS                 4.0    -0.175  -0.931    0.582  0.323  17988.0  -0.542  0.851     \n",
      "11  PB - HB                 4.0    -0.059  -0.815    0.697  0.323  17988.0  -0.183  0.982     \n",
      "12  RS - HB                 4.0     0.116  -0.641    0.872  0.323  17988.0   0.359  0.932     \n",
      "13  PB - RS                 5.0    -0.123  -0.879    0.634  0.323  17988.0  -0.380  0.923     \n",
      "14  PB - HB                 5.0    -0.057  -0.813    0.699  0.323  17988.0  -0.177  0.983     \n",
      "15  RS - HB                 5.0     0.066  -0.691    0.822  0.323  17988.0   0.203  0.977     )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amega\\Git\\significance_analysis\\.venv_3_10_0\\lib\\site-packages\\pandas\\core\\internals\\blocks.py:1491: RuntimeWarning: invalid value encountered in multiply\n",
      "  values = self.values.round(decimals)  # type: ignore[union-attr]\n"
     ]
    }
   ],
   "source": [
    "# Trying to get grouping on ordinal variables:\n",
    "\n",
    "data=fig5_df.copy(); system_id=\"algorithm\";fidelity_var=\"used_fidelity\"\n",
    "data=data.loc[data[fidelity_var]<6]\n",
    "data[f\"{fidelity_var}_group\"]=data[fidelity_var].apply(lambda x: str(x))\n",
    "from pymer4 import Lmer\n",
    "\n",
    "mod1=Lmer(formula=f\"value~{system_id}+{fidelity_var}+{fidelity_var}_group+{system_id}:{fidelity_var}_group+(1|benchmark)\",data=data)\n",
    "factor_list = {system_id: list(data[system_id].unique())}\n",
    "factor_list[f\"{fidelity_var}_group\"] = list(data[f\"{fidelity_var}_group\"].unique())\n",
    "print(factor_list)\n",
    "mod1.fit(factors=factor_list, REML=False, summarize=False, verbose=True)\n",
    "print(mod1.summary())\n",
    "print(mod1.fixef)\n",
    "print(mod1.post_hoc(marginal_vars=\"algorithm\",grouping_vars=f\"{fidelity_var}_group\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plot_1\u001b[38;5;241m=\u001b[39mbt_plot([[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m25\u001b[39m]],rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,dataset\u001b[38;5;241m=\u001b[39mfig5_df\u001b[38;5;241m.\u001b[39mloc[(fig5_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m&\u001b[39m(fig5_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbench_prior\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLC-167190_at25\u001b[39m\u001b[38;5;124m\"\u001b[39m)],algorithm_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m,budget_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused_fidelity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mplot_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_row\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mglobality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m plot_1\u001b[38;5;241m.\u001b[39mchange_row(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m,globality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrel_rank\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m plot_1\u001b[38;5;241m.\u001b[39mchange_row(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m,globality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Amega\\Git\\significance_analysis\\Thesis\\models.py:905\u001b[0m, in \u001b[0;36mbt_plot.change_row\u001b[1;34m(self, row, lmem_formula, globality, loss)\u001b[0m\n\u001b[0;32m    898\u001b[0m         autorank_data \u001b[38;5;241m=\u001b[39m convert_to_autorank(\n\u001b[0;32m    899\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf_slices[cell_n],\n\u001b[0;32m    900\u001b[0m             algorithm_variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malgorithm,\n\u001b[0;32m    901\u001b[0m             value_variable\u001b[38;5;241m=\u001b[39mloss,\n\u001b[0;32m    902\u001b[0m             budget_variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbudget,\n\u001b[0;32m    903\u001b[0m         )\n\u001b[0;32m    904\u001b[0m         autorank_res \u001b[38;5;241m=\u001b[39m autorank(autorank_data)\n\u001b[1;32m--> 905\u001b[0m         \u001b[43mcd_diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mautorank_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcell_n\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m globality:\n",
      "File \u001b[1;32mc:\\Users\\Amega\\Git\\significance_analysis\\Thesis\\models.py:823\u001b[0m, in \u001b[0;36mcd_diagram\u001b[1;34m(result, reverse, width, ax)\u001b[0m\n\u001b[0;32m    821\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cd_n,cdv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(cd):\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reverse:\n\u001b[1;32m--> 823\u001b[0m         begin, end \u001b[38;5;241m=\u001b[39m rankpos(lowv), rankpos(\u001b[43mlowv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcdv\u001b[49m)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    825\u001b[0m         begin, end \u001b[38;5;241m=\u001b[39m rankpos(highv), rankpos(highv \u001b[38;5;241m-\u001b[39m cdv)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "plot_1=bt_plot([[0,25]],rows=4,dataset=fig5_df.loc[(fig5_df[\"algorithm\"]!=\"RS\")&(fig5_df[\"bench_prior\"]==\"LC-167190_at25\")],algorithm_var=\"algorithm\",budget_var=\"used_fidelity\")\n",
    "plot_1.change_row(0,None,globality=False,loss=\"value\")\n",
    "plot_1.change_row(1,f\"algorithm\",globality=False,loss=\"rel_rank\")\n",
    "plot_1.change_row(2,f\"algorithm\",globality=False,loss=\"value\")\n",
    "plot_1.change_row(3,f\"algorithm\",globality=False,loss=\"regret\")\n",
    "plot_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Name         Var        Std\n",
    "algorithm      benchmarkJAHS-C10  135.685244  11.648401\n",
    "algorithm       benchmarkJAHS-CH   52.165100   7.222541\n",
    "algorithm       benchmarkJAHS-FM   35.441897   5.953310\n",
    "algorithm     benchmarkLC-126026    0.254739   0.504716\n",
    "algorithm     benchmarkLC-167190    0.125850   0.354753\n",
    "algorithm     benchmarkLC-168330    0.016117   0.126953\n",
    "algorithm     benchmarkLC-168910    0.046651   0.215989\n",
    "algorithm     benchmarkLC-189906    0.143302   0.378552\n",
    "algorithm  benchmarkPD1-Cifar100    0.045919   0.214287\n",
    "algorithm  benchmarkPD1-ImageNet    0.065133   0.255213\n",
    "algorithm      benchmarkPD1-LM1B    0.017931   0.133906\n",
    "algorithm       benchmarkPD1-WMT    0.016730   0.129346\n",
    "Residual                           16.604217   4.074827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ first case: No seed influence ############\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "model_builder.__init__() got an unexpected keyword argument 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;66;03m# if test_results[\"p\"]<0.05:\u001b[39;00m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;66;03m#     print(f\"Feature {feature} is significant\")\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m############ first case: No seed influence ############\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m builder\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43msystem_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malgorithm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfactors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malgorithm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m builder\u001b[38;5;241m.\u001b[39mtest_feature(feature\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m ############ second case: Linear seed influence ############\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: model_builder.__init__() got an unexpected keyword argument 'features'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def test_feature(self,feature:str):\n",
    "    formulae=[feature,f\"(1|{feature})\",\"(0+\"+f\"{self.exploratory_var}|{feature})\",\"(1+\"+f\"{self.exploratory_var}|{feature})\"]\n",
    "\n",
    "    base_formula=self.loss_formula+f\" + {self.exploratory_var} + \".join([self.effect_types[feature] for feature in self.included_features])\n",
    "    base_model=model(formula=base_formula,data=self.df,factor_list=[\"algorithm\"],dummy=False)\n",
    "    base_model=model(formula=base_formula,data=self.df,factor_list=[factor for factor in self.factors if factor in self.included_features],dummy=False)\n",
    "    new_models:list[Lmer]=[]\n",
    "    for formula in formulae:\n",
    "\n",
    "        new_formula=self.loss_formula+\" + \".join([self.effect_types[feature] for feature in self.included_features])+f\" + {formula}\"\n",
    "        # print(new_formula)\n",
    "                                                \n",
    "        new_models.append(model(formula=new_formula,data=self.df,factor_list=[factor for factor in self.factors if factor in self.included_features+[feature]],dummy=False))\n",
    "        print(new_models[-1].summary())\n",
    "        if type(new_models[-1]) is Lmer:\n",
    "            print(new_models[-1].ranef)\n",
    "            print(new_models[-1].ranef_var)\n",
    "        \n",
    "    test_results=glrt(base_model,new_models[0],names=[\"baseline\",f\"baseline + {feature}\"],returns=True)\n",
    "    test_results=glrt(new_models[0],new_models[1],names=[f\"baseline + {feature}\",f\"baseline + {formulae[1]}\"],returns=True)\n",
    "    test_results=glrt(new_models[0],new_models[2],names=[f\"baseline + {feature}\",f\"baseline + {formulae[2]}\"],returns=True)\n",
    "    test_results=glrt(new_models[0],new_models[3],names=[f\"baseline + {feature}\",f\"baseline + {formulae[3]}\"],returns=True)\n",
    "    test_results=glrt(new_models[1],new_models[2],names=[f\"baseline + {formulae[1]}\",f\"baseline + {formulae[2]}\"],returns=True)\n",
    "    test_results=glrt(new_models[1],new_models[3],names=[f\"baseline + {formulae[1]}\",f\"baseline + {formulae[3]}\"],returns=True)\n",
    "    test_results=glrt(new_models[2],new_models[3],names=[f\"baseline + {formulae[2]}\",f\"baseline + {formulae[3]}\"],returns=True)\n",
    "\n",
    "model_builder.test_feature=test_feature\n",
    "\n",
    "        # if test_results[\"p\"]<0.05:\n",
    "        #     print(f\"Feature {feature} is significant\")\n",
    "\n",
    "print(\"############ first case: No seed influence ############\")\n",
    "builder=model_builder(random_df,system_var=\"algorithm\",features=[\"seed\"],factors=[\"seed\",\"algorithm\"])\n",
    "builder.test_feature(feature=\"seed\")\n",
    "\n",
    "print(\"\\n ############ second case: Linear seed influence ############\")\n",
    "builder2=model_builder(algo_by_seed_df,system_var=\"algorithm\",features=[\"seed\"],factors=[\"seed\",\"algorithm\"])\n",
    "builder2.test_feature(feature=\"seed\")\n",
    "\n",
    "print(\"\\n ############ third case: Modulo seed influence ############\")\n",
    "builder3=model_builder(seed2_df,system_var=\"algorithm\",features=[\"seed\"],factors=[\"seed\",\"algorithm\"])\n",
    "builder3.test_feature(feature=\"seed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_3_10_0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
